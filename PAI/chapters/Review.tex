\section{Review of useful concepts and Introduction}

\subsection{Usefull math}
$\varphi$ is convex $\Rightarrow \varphi(\mathbb{E}[X]) \leq  \mathbb{E}[\varphi(X)]$\\
\textbf{Hoeffding}: $Z_1,\dots iid, Z_i\in [0,C], \mathbb{E}[Z_i]=\mu$\\
$\Rightarrow P\left(\left\lvert \mu - \frac{1}{n}\sum_{i=1}^n Z_i\right\rvert > \epsilon \right)\leq 2 \exp(-2n\frac{\epsilon^2}{C})\leq \delta$\\
$\Rightarrow n \geq \frac{C}{2\epsilon^2}\log\frac{2}{\delta}$\\
\textbf{Robbins Monro} $\alpha_t \xrightarrow{RM} 0$: $\sum \alpha_t = \infty, \sum \alpha_t^2 < \infty$ 
\subsection{Multivariate Gaussian}
%$\sigma =$ covariance matrix, $\mu$ = mean\\
$f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$

Suppose we have a Gaussian random vector
\(
\begin{bmatrix} 
    X_A \\ 
    X_B 
\end{bmatrix} 
\sim \mathcal{N}\left(
\begin{bmatrix}
    {\mu_A} \\ 
    {\mu_B}
\end{bmatrix},
\begin{bmatrix} 
    \Sigma_{AA}&\Sigma_{AB} \\
    \Sigma_{BA}&\Sigma_{BB} 
\end{bmatrix}\right)
\Rightarrow X_A\vert X_B=x_B \sim \mathcal{N}\left(
    \mu_A+\Sigma_{AB}\Sigma^{-1}_{BB}(x_B-\mu_B) , 
    \Sigma_{AA}-\Sigma_{AB}\Sigma^{-1}_{BB}\Sigma_{BA}
\right)
\)


\subsection{Information Theory elements:}
\textbf{Entropy:} $H(X)\doteq -\mathbb{E}_{x\sim p_X}\left[\log{p_X(x)}\right]$\\
$H(X\vert Y)\doteq -\mathbb{E}_{(x,y)\sim p_{(X,Y)}}\left[\log{p_{Y\vert X}(y\vert x)}\right]$\\
if $X\sim\mathcal{N}(\mu,\Sigma) \Rightarrow H(X) = \frac{1}{2}\log{\left[{(2\pi e)}^d \det{(\Sigma)}\right]}$
\textbf{Chain Rule:} $H(X, Y) = H(Y\vert X)+H(X)$\\
\textbf{Mutual Info:} $I(X, Y)\doteq KL(p_{(X,Y)}\vert\vert p_Xp_Y)$\\
$I(X, Y)=H(X)-H(X\vert Y)$\\
if $X\sim\mathcal{N}(\mu, \Sigma),\; Y=X+\epsilon, \; \epsilon\sim\mathcal{N}(0,\sigma^2 I)$:\\
then $I(X, Y)=\frac{1}{2}\log{\left[\det{(I+\frac{1}{\sigma^2}\Sigma)}\right]}$


\subsection{Kullback-Leiber divergence}
$KL(p\vert\vert q)=\mathbb{E}_p\left[\log\frac{p(x)}{q(x)}\right]$\\
if $p_0\sim \mathcal{N}(\mu_0,\Sigma_0),\; p_1\sim \mathcal{N}(\mu_1,\Sigma_1) \Rightarrow KL(p_0\vert\vert p_1)$\\
{\scriptsize$ = \frac{1}{2}\left(tr\left(\Sigma_1^{-1}\Sigma_0\right) + (\mu_1-\mu_0)^T\Sigma_1^{-1}(\mu_1-\mu_0)-k+\log\frac{\vert\Sigma_1\vert}{\vert\Sigma_0\vert}\right)$}\\
$\hat q = \argmin_q KL(p\vert\vert q) \Rightarrow$ overconservative\\
$\hat q = \argmin_q KL(q\vert\vert p) \Rightarrow$ overconfident
