\section{Non Parametric RL}
It is an MDP with unknown $p(x'\vert x,a)$ and $r(x,a)$

\subsection{Model-based RL}
From all steps $X_{t+1},R_t \vert X_{t}, A_{t}$ we can learn:\\
$p(x'\vert x,a)\simeq \hat{p}_{x'\vert x, a} = \frac{Count(X_{t+1}=x',\; X_t=x,\;A_t =a)}{Count(X_t=x,\; A_t = a)}$\\
$r(x,a)\simeq \hat{r}_{x,a}= \frac{1}{Count(X_t=x,\; A_t=a)}\sum_{t|X_t=x,\; A_t=a}R_t$\\
How to chose $a_t$?

\subsubsection{$\epsilon$-greedy (On-Policy)}
With probability $\epsilon$, pick random action.\\
With probability $1-\epsilon$, pick $a = \argmax Q(x,a)$.\\
\textbf{Oss:} $Q$ is caclulated from $(\hat{p}, \hat{r})$\\
\textbf{Th:} If $\epsilon_t\xrightarrow{RM}0$ then $(\hat{r},\hat{p})\xrightarrow{a.s.}(r,p)$

\subsubsection{Softmax (On-Policy)}
Draw $a \sim q(a\vert x) = \text{softmax}\frac{Q(x,a)}{\tau}$\\
If $\tau \uparrow $ it means I trust less $Q$

\subsubsection{$R_{max}$ algorithm (On-Policy)}
We add a fairy state $x^*$\\
\begin{algorithm}[H]
    \SetKwInput{kwInit}{init}
    \kwInit{$r(x,a) = R_{max} \; \forall x\in \mathcal{X}\cup \left\{x^*\right\},a\in\mathcal{A}$}
    \kwInit{$p(x^*\vert x, a) = 1 \; \forall x\in \mathcal{X},a\in\mathcal{A}$}
    \kwInit{$\pi = $ optimal policy w.r.t. $p$, $r$}
    \Repeat{}{  
        Execute $\pi$ and get $x_{t+1}$ and $r_t$\\
        Update belief of $r(x_t,\pi(x_t))$ and $p(x_{t+1}\vert x_t,\pi(x_t))$\\
        If obeserved 'enough' in $(x,a)$ recompute $\pi$ using the updated belief only in $(x,a)$
    }
\end{algorithm}
\textbf{'Enough'?} See Hoeffding's inequality \\
($\hat{p}\in [0, 1],\; \hat{r}\in [0, R_{max}]$).\\
\textbf{PAC bound:} With probability $1-\delta$, $R_{max}$ will reach an $\epsilon$-optimal policy in a number of steps that is polynomial in $|X|, |A|, T, 1/\epsilon$ and $log(1/\delta)$. Memory $O(|X|^2|A|)$. 

\subsection{Model-free RL}
Learn $\pi^*$ only via $V^*$ or $Q^{V^*}$

\subsubsection{TD-learning (On-Policy)}
Given a policy $\pi$ we want to learn $V^\pi$\\
$V^\pi(x)=\mathbb{E}_{R\sim r(x,\pi(x)), X'\sim p(\cdot\vert x, \pi(x))}\left[R + \gamma V^\pi(X')\right]$\\
After seeing $(x_{t+1}, r_t \vert x_t, \pi(x_t))$ we update: \\
$V_{t+1}(x_t)\gets (1-\alpha_t)V_{t}(x_t) + \alpha_t(r_t + \gamma V_t^\pi(x_{t+1}))$\\
Where $\alpha_t$ is a regulizer term (only 1 samlple)
\textbf{Th:} If $\alpha_t\xrightarrow{RM}0$ then $V\xrightarrow{a.s.}V^\pi$

\subsubsection{Q-learning (Off Policy)}
Given experience we want to learn $Q^* = Q^{V^*}$\\
$Q^*(x,a) = \mathbb{E}_{\substack{R\sim r(x,\pi(x)) \\ X'\sim p(\cdot\vert x, \pi(x))}}\left[R+\gamma \max_{a'} Q^*(X',a')\right]$\\
After seeing $(x_{t+1}, r_t \vert x_t, a_t)$ we update: \\
$Q(x_t,a_t) ${\scriptsize $\gets (1-\alpha_t)Q(x_t,a_t) + \alpha_t(r_t+\gamma \max_{a'}Q(x_{t+1}, a'))$}\\
% $V^*(x)=\underset{a}{max}Q*(x,a)$\\
\textbf{Th:} If $\alpha_t\xrightarrow{RM}0$ then $Q\xrightarrow{a.s.}Q^{*}$\\
\textbf{Optimistic Q learning:}\\
Initialize: $Q(x,a)=\frac{R_{max}}{1-\gamma}\prod_{t=1}^{T_{init}}(1-\alpha_t)^{-1}$\\
Same convergence time as with $R_{max}$. Memory $O(|X||A|)$. Comp: $O(|A|)$.

\section{Parametric RL}
\subsection{Parametric TD-learning}
\subsubsection{TD-learinging as SGD}
TD-learing = 1 sample ($x', r \vert x, \pi(x)$) SGD on:\\
$\bar{l}_2(V;x,r) = \frac{1}{2}\left(V-r-\gamma\mathbb{E}_{x'\sim p(\cdot\vert x, \pi(x))}\left[\hat{V}^\pi(x')\right]\right)^2$\\
1 sample estimate of $\nabla_V \bar{l}_2 = \delta = V-r-\gamma \hat{V}^\pi(x')$\\
$\Rightarrow V\gets V - \alpha_t \delta$ where $V = \hat{V}^\pi(x)$

\subsubsection{TD-parametric}
If $\hat{V}^\pi(x) = V(x,\theta)$ then: \\
$\delta = \left[ V(x;\theta)-r-\gamma V(x';\theta_{old})\right] \nabla_\theta V(x,\theta)$

\subsection{Parametric Q-learining}
$\delta (\theta, \theta_{old}) = \left(Q(x,a;\theta)-r-\gamma\max_{a'} Q(x',a';\theta_{old})\right)$\\
We don't differiantiate with regard to $\theta_{old}$\\
The SGD step is:
$\theta \gets \theta - \alpha_t \delta(\theta, \theta) \nabla_\theta Q(x,a;\theta)$

\textbf{Deep Q Networks (DQN):} Version of Q-learning where we update $Q$ only each batch:\\
{\scriptsize $L(\theta) = \sum_{(x,a,r,x')\in\mathcal{D}}\left(r+\gamma\max_{a'}Q(x',a';\theta_{old})-Q(x,a;\theta)\right)^2$}
\textbf{Double DQN (better):} \\
{\scriptsize $L(\theta) = \sum_{(x,a,r,x')\in\mathcal{D}}\left(r+\gamma Q(x',a^*(\theta);\theta_{old})-Q(x,a;\theta)\right)^2$}\\
where: $a^*(\theta) \doteq \argmax_{a'}Q(x',a'; \theta)$\\
\subsection{Policy-Search method}



