\section{Non Parametric RL}
It is an MDP with unknown $P(x'\vert x,a)$ and $r(x,a)$

\subsection{Model-based RL}
From all steps $X_{t+1},R_t \vert X_{t}, A_{t}$ we can learn:\\
$P(x'\vert x,a)\simeq \hat{p}_{x'\vert x, a} = \frac{Count(X_{t+1}=x',\; X_t=x,\;A_t =a)}{Count(X_t=x,\; A_t = a)}$\\
$r(x,a)\simeq \hat{r}_{x,a}= \frac{1}{Count(X_t=x,\; A_t=a)}\sum_{t|X_t=x,\; A_t=a}R_t$\\
How to chose $a_t$?

\subsubsection{$\epsilon$-greedy (On-Policy)}
$P(rand(a)) = \epsilon$, $P(\argmax Q(x,a)) = 1 - \epsilon$\\
\textbf{Oss:} $Q$ is caclulated from $(\hat{p}, \hat{r})$\\
\textbf{Th:} If $\epsilon_t\xrightarrow{RM}0$ then $(\hat{r},\hat{p})\xrightarrow{a.s.}(r,p)$

\subsubsection{Softmax (On-Policy)}
Draw $a \sim q(a\vert x) = \text{softmax}\frac{Q(x,a)}{\tau}$\\
If $\tau \uparrow $ it means I trust less $Q$

\subsubsection{$R_{max}$ algorithm (On-Policy)}
We add a fairy state $x^*$\\
\begin{algorithm}[H]
    \SetKwInput{kwInit}{init}
    \kwInit{$r(x,a) = R_{max} \; \forall x\in \mathcal{X}\cup \left\{x^*\right\},\forall a\in\mathcal{A}$}
    \kwInit{$P(x^*\vert x, a) = 1 \; \forall x\in \mathcal{X},\forall a\in\mathcal{A}$}
    \kwInit{$\pi = $ optimal policy w.r.t. $p$, $r$}
    \Repeat{}{  
        Execute $\pi$ and get $x_{t+1}$ and $r_t$\\
        Update belief of $r(x_t,\pi(x_t))$ and $p(x_{t+1}\vert x_t,\pi(x_t))$\\
        If observed 'enough' in $(x,a)$ recompute $\pi$ using the updated belief only in $(x,a)$
    }
\end{algorithm}
\textbf{'Enough'?} See Hoeffding's inequality \\
($\hat{p}\in [0, 1],\; \hat{r}\in [0, R_{max}]$).\\
\textbf{PAC bound:} With probability $1-\delta$, $R_{max}$ will reach an $\epsilon$-optimal policy in a number of steps that is polynomial in $|X|, |A|, T, 1/\epsilon$ and $log(1/\delta)$. Memory $O(|X|^2|A|)$. 

\subsection{Model-free RL}
Learn $\pi^*$ only via $V^*$ or $Q^{V^*}$

\subsubsection{TD-learning (On-Policy)}
Given a policy $\pi$ we want to learn $V^\pi$\\
$V^\pi(x)=\mathbb{E}_{R\sim r(x,\pi(x)), X'\sim p(\cdot\vert x, \pi(x))}\left[R + \gamma V^\pi(X')\right]$\\
After seeing $(x_{t+1}, r_t \vert x_t, \pi(x_t))$ we update: \\
$V_{t+1}(x_t)\gets (1-\alpha_t)V_{t}(x_t) + \alpha_t(r_t + \gamma V_t^\pi(x_{t+1}))$\\
Where $\alpha_t$ is a regulizer term (only 1 samlple)
\textbf{Th:} If $\alpha_t\xrightarrow{RM}0$ then $V\xrightarrow{a.s.}V^\pi$

\subsubsection{Q-learning (Off Policy)}
Given experience we want to learn $Q^* = Q^{V^*}$\\
$Q^*(x,a) = \mathbb{E}_{\substack{R\sim r(x,\pi(x)) \\ X'\sim p(\cdot\vert x, \pi(x))}}\left[R+\gamma \max_{a'} Q^*(X',a')\right]$\\
After seeing $(x_{t+1}, r_t \vert x_t, a_t)$ we update: \\
$Q(x_t,a_t) ${\scriptsize $\gets (1-\alpha_t)Q(x_t,a_t) + \alpha_t(r_t+\gamma \max_{a'}Q(x_{t+1}, a'))$}\\
% $V^*(x)=\underset{a}{max}Q*(x,a)$\\
\textbf{Th:} If $\alpha_t\xrightarrow{RM}0$ then $Q\xrightarrow{a.s.}Q^{*}$\\
\textbf{Optimistic Q learning:}\\
Initialize: $Q(x,a)=\frac{R_{max}}{1-\gamma}\prod_{t=1}^{T_{init}}(1-\alpha_t)^{-1}$\\
Same convergence time as with $R_{max}$. Memory $O(|X||A|)$. Comp: $O(|A|)$.

\section{Parametric Model Free RL}
\subsection{Parametric TD-learning}
\subsubsection{TD-learinging as SGD}
TD-learing = 1 sample ($x', r \vert x, \pi(x)$) SGD on:\\
$\bar{l}_2(V;x,r) = \frac{1}{2}\left(V-r-\gamma\mathbb{E}_{x'\sim p(\cdot\vert x, \pi(x))}\left[\hat{V}^\pi(x')\right]\right)^2$\\
1 sample estimate of $\nabla_V \bar{l}_2 = \delta = V-r-\gamma \hat{V}^\pi(x')$\\
$\Rightarrow V\gets V - \alpha_t \delta$ where $V = \hat{V}^\pi(x)$

\subsubsection{TD-parametric}
If $\hat{V}^\pi(x) = V(x,\theta)$ then: \\
$\delta = \left[ V(x;\theta)-r-\gamma V(x';\theta_{old})\right] \nabla_\theta V(x,\theta)$

\subsection{Parametric Q-learining}
$\delta (\theta, \theta_{old}) = \left(Q(x,a;\theta)-r-\gamma\max_{a'} Q(x',a';\theta_{old})\right)$\\
We don't differiantiate with regard to $\theta_{old}$\\
The SGD step is:
$\theta \gets \theta - \alpha_t \delta(\theta, \theta) \nabla_\theta Q(x,a;\theta)$

\textbf{Deep Q Networks (DQN):} Version of Q-learning where we update $Q$ only each batch:\\
{\scriptsize $L(\theta) = \sum_{(x,a,r,x')\in\mathcal{D}}\left(r+\gamma\max_{a'}Q(x',a';\theta_{old})-Q(x,a;\theta)\right)^2$}
\textbf{Double DQN (better):} \\
{\scriptsize $L(\theta) = \sum_{(x,a,r,x')\in\mathcal{D}}\left(r+\gamma Q(x',a^*(\theta);\theta_{old})-Q(x,a;\theta)\right)^2$}\\
where: $a^*(\theta) \doteq \argmax_{a'}Q(x',a'; \theta)$

\subsection{Policy-Search method}
$\pi(x) = \pi(x;\theta); \; r(\tau) = \sum_{t=0}^T\gamma^t r(x_t,a_t)$\\
$J(\theta)\doteq J(\pi(\cdot;\theta))= \mathbb{E}_{\tau \sim \pi_\theta}\left[r(\tau)\right] \simeq \frac{1}{m}\sum_{i=1}^m r(\tau^{(i)})$ \\
with $\pi_\theta (\tau) := p(x_0)\prod_{t=1}^T\pi(a_t\vert x_t; \theta)p(x_{t+1}\vert x_t,a_t)$\\
$\hat{\theta} = \argmax_\theta J(\theta) \Rightarrow$ SGD 

\subsubsection{Theory results:}
{\scriptsize $\nabla_\theta J(\theta) = \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}\left[r(\tau)\right]=\mathbb{E}_{\tau \sim \pi_\theta}\left[r(\tau)\nabla_\theta \log{\pi_\theta(\tau)}\right] =: \clubsuit $}\\
{ $ \clubsuit = \mathbb{E}_{\tau\sim\pi_\theta}\left[r(\tau)\sum_{t=0}^T \nabla_\theta \log\pi(a_t\vert x_t;\theta )\right]$}\\
{$ \clubsuit = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum\limits_{t=0}^T(r(\tau)-b(\tau_{0:t-1})) \nabla_\theta \log\pi(a_t\vert x_t;\theta )\right] $}

\subsubsection{REINFORCE}
$b(\tau_{0:t-1}) = \sum_{t'=0}^{t-1}\gamma^{t'}r_{t'};\; r(\tau) - b(\tau_{0:t-1}) = \gamma^t G_t$ \\
Let $G_t = \sum_{t'=t}^{T}\gamma^{t'-t}r_{t'} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots$ \\
$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T\gamma^t G_t\nabla_\theta \log\pi(a_t\vert x_t;\theta )\right]  $\\
SGD:  $\theta \gets \theta + \eta_t \gamma^tG_t \nabla_\theta \pi(a_t\vert x_t; \theta)\;\; \forall t = 1:T$ 
\textbf{Oss:} $G_t \gets G_t - \frac{1}{T}\sum_{t'=0}^{T-1} G_t' \Rightarrow$ Less Variance


\subsection{Actor Critic}
After seeing $(x,a,r,x')$: \\
$\theta_\pi \gets \theta_\pi + \eta_t Q(x,a,\theta_Q)\nabla_{\theta_\pi}\log \pi(a\vert x; \theta_\pi)$\\
$\theta_Q \gets \theta_Q - \eta_t\delta \nabla_{\theta_Q}Q(x,a;\theta_Q)$\\
$\qquad \delta = Q(x,a;\theta_Q)-r-\gamma Q(x',\pi(x',\theta_\pi),\theta_Q)$

\subsubsection{Theory justification:}
$\mathbb{E}_{\tau_{t+1:\infty}\sim \pi_\theta}\left[G_t \vert a_t,x_t\right] = Q^{\pi_\theta}(x_t, a_t)$\\
{\scriptsize $\nabla_{\theta_\pi} J(\theta_\pi)= \mathbb{E}_{\tau \sim \pi_{\theta_\pi}}\left[\sum_{t=0}^T\gamma^t \nabla_{\theta_\pi} \log\pi(a_t\vert x_t;\theta_\pi ) Q(x_t,a_t)\right] $}\\
{\scriptsize $\nabla_{\theta_\pi} J(\theta_\pi)= \mathbb{E}_{\tau \sim \pi_{\theta_\pi},x\sim \rho(\cdot)}\left[\nabla_{\theta_\pi} \log\pi(a_t\vert x_t;\theta_\pi ) Q(x_t,a_t)\right] $}\\
where $\rho(x) = \sum_{t=0}^\infty \gamma^t p(x_t=x)$.

\subsubsection{Advantage Function Baseline}
We can us $V(x;\theta_V)$ as baseline for $Q(x,a;\theta_Q)$\\
$A^\pi(x,a) \doteq Q^\pi(x,a)-V^\pi(x)$\\
\textbf{Oss:} $\forall\pi \forall x \max_a A^\pi(x,a)\geq 0$\\
\textbf{Oss:} $\pi \text{ optimal } \forall x  \;A^\pi(x,\pi(x))\leq 0$.

\subsubsection{Off policy variation (DDPG)}
{\scriptsize $\hat{\theta}_\pi = \argmax J(\theta_\pi); \; J(\theta_\pi)=\mathbb{E}_{x\sim\mu}\left[Q(x,\pi(x;\theta_\pi);\theta_Q)\right]$}\\
where $\mu$ visits all states. SGD can now be done in Batches and Off-policy also for $\theta_\pi$:\\
$\theta_\pi \gets \theta_\pi + \eta \nabla_{\theta_\pi} \frac{1}{|B|}\sum_{(x,a,r,x')\in B}Q(x,\pi(x,\theta_\pi);\theta_Q)$\\
\textbf{Oss:}
$\pi$ randomized $\Rightarrow$ reparametrization trick.\\
$J_\lambda (\theta) = J(\theta) + \lambda H(\pi_\theta)$ to encourage exploration


\section{Parametric Model Based RL}


\subsection{Planning}
$J(a_{0:\infty}) := \sum_{t=0}^\infty \gamma^t r(x_t,a_t)$

\subsubsection{Reciding Horizon (MPC)}
$\max\limits_{a_{t:t+H-1}}\sum_{\tau = t}^{t+H-1}\gamma^{\tau - t}r_\tau(x_\tau, a_\tau) \text{ s.t. } x_{\tau + 1} = f(x_\tau, a_\tau)$\\
And then we carry out action $a_t$.\\
\textbf{$\nabla \uparrow$:} $\nabla$ of $f\circ \dots \circ f$ easy vanishes or explodes.
\textbf{Random shooting:} Pick some set of actions at random; chose the 1st action of the best one.

\subsubsection{Value Function estimates}
We look at value function after the horizon H: \\
We maximize $J_H(a_{t:t+H-1}) \coloneqq\\ \sum_{\tau = t}^{t+H-1}\gamma^{\tau - t}r_\tau(x_\tau (a_{t:\tau-1}), a_\tau) + \gamma^H V(x_{t+H})$\\
\textbf{Microth:} If $V$ is correct then the chosen action is optimal $\forall H$\\
\textbf{Oss:} Value can estimated with off-policy techniques (TD-learing)

\subsubsection{Planning (stochastic model)} \label{pl}
{\scriptsize $\mathbb{E}_{x_{t+1:t+H}}\left[ \sum\limits_{\tau = t}^{t+H-1}\gamma^{\tau - t}r_\tau(x_\tau, a_\tau) + \gamma^H V(x_{t+H})\vert a_{t:t+H-1}\right]$}\\
maximise this doing SGD by sampling or random shooting.

\subsubsection{Parametric policy (stoch. model)}
{\scriptsize $\mathbb{E}_{x_{t+1:t+H}}\left[ \sum\limits_{\tau = t}^{t+H-1}\gamma^{\tau - t}r_\tau(x_\tau, \pi_\theta(x_\tau)) + \gamma^H Q(x_{t+H}, \pi_\theta(x_{t+H}))\vert \theta\right]$}\\
\textbf{Oss:} H=0 $\Rightarrow$ DDPG objective

\subsection{Model Learning}
\textbf{Insight:} Markov $\Rightarrow \mathcal{D} = \left\{(x_i,a_i,r_i,x_{i+1})_i\right\}_{i=1:t}$ is a conditional $\perp$ dataset\\
Learn $f, \mathcal{R}\text{ s.t.} \;\; x_{i+1} \gets f(x_i,a_i);\quad r_i \gets \mathcal{R}(x_i,a_i)$ 

\subsubsection{Neural Networks}
$x_{t+1} \sim \mathcal{N}(\mu(x_t, a_t, \theta), \Sigma(x_t, a_t, \theta))$\\
Where $\Sigma = CC^T,\; C$ lower triangular. \\
\textbf{Pittfall:} using MAP estimate we don't capture epistemic uncertainty $\Rightarrow$ Bayesian Learning\\
\textbf{Greedy exploitation:}\\
\begin{algorithm}[H]
    \Repeat(){}{
        plan $\pi$ to maximize $\mathbb{E}_{f\sim p(\cdot\vert \mathcal{D})}J_H(\pi,f)$\\
        rollout $\pi$ to collect more data\\
        update $p(\cdot\vert\mathcal{D})$
    }
\end{algorithm}
To find $\pi$ we use SGD or Trajectory samling (sampling also $f\sim p(\cdot\vert \mathcal{D})$) 

\subsubsection{Exploration?}
\textbf{Thompson:} {\scriptsize $ \mathbb{E}_{f\sim p(\cdot\vert \mathcal{D})}J_H(\pi,f) \simeq J_H(\pi,\hat(f)),\; \hat{f}\sim p(\cdot\vert \mathcal{D})$}\\
\textbf{Optimism:} Let $M(\mathcal{D})$ be set of most plausible models given $\mathcal{D}$. then:\\
$\mathbb{E}_{f\sim p(\cdot\vert \mathcal{D})}J_H(\pi,f) \simeq \max_{f\in M(\mathcal{D})} J_H(\pi,f)$\\
We do Greedy exploitation but joint maximization $max_\pi\max_{f\in M(\mathcal{D})} J_H(\pi,f)$ is hard: \\H-URCL: 
$\tilde{f}(x,a) = \mu(x,a)+\beta_{t-1}\Sigma (x,a)\eta(x,a)$.
then $\pi_t^{H-URCL}=\argmax_\pi J_H(\tilde{f}, \pi)$

\subsection{Safe Exploration planning}
With confidence bounds I make $1-\delta$ sure in H step I can be safe. Issue, long term dependency\\
\textbf{Liapunov:} I make sure I can reach a Liapunov state












