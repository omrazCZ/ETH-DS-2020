\section{Non Parametric RL}
It is an MDP with unknown $p(x'\vert x,a)$ and $r(x,a)$

\subsection{Model-based RL}
From all steps $X_{t+1},R_t \vert X_{t}, A_{t}$ we can learn:\\
$p(x'\vert x,a)\simeq \hat{p}_{x'\vert x, a} = \frac{Count(X_{t+1}=x',\; X_t=x,\;A_t =a)}{Count(X_t=x,\; A_t = a)}$\\
$r(x,a)\simeq \hat{r}_{x,a}= \frac{1}{Count(X_t=x,\; A_t=a)}\sum_{t|X_t=x,\; A_t=a}R_t$\\
How to chose $a_t$?

\subsubsection{$\epsilon$-greedy (On-Policy)}
With probability $\epsilon$, pick random action.\\
With probability $1-\epsilon$, pick $a = \argmax Q(x,a)$.\\
\textbf{Oss:} $Q$ is caclulated from $(\hat{p}, \hat{r})$\\
\textbf{Th:} If $\epsilon_t\xrightarrow{RM}0$ then $(\hat{r},\hat{p})\xrightarrow{a.s.}(r,p)$

\subsubsection{Softmax (On-Policy)}
Draw $a \sim q(a\vert x) = \text{softmax}\frac{Q(x,a)}{\tau}$\\
If $\tau \uparrow $ it means I trust less $Q$

\subsubsection{$R_{max}$ algorithm (On-Policy)}
We add a fairy state $x^*$\\
\begin{algorithm}[H]
    \SetKwInput{kwInit}{init}
    \kwInit{$r(x,a) = R_{max} \; \forall x\in \mathcal{X}\cup \left\{x^*\right\},a\in\mathcal{A}$}
    \kwInit{$p(x^*\vert x, a) = 1 \; \forall x\in \mathcal{X},a\in\mathcal{A}$}
    \kwInit{$\pi = $ optimal policy w.r.t. $p$, $r$}
    \Repeat{}{  
        Execute $\pi$ and get $x_{t+1}$ and $r_t$\\
        Update belief of $r(x_t,\pi(x_t))$ and $p(x_{t+1}\vert x_t,\pi(x_t))$\\
        If obeserved 'enough' in $(x,a)$ recompute $\pi$ using the updated belief only in $(x,a)$
    }
\end{algorithm}
\textbf{'Enough'?} See Hoeffding's inequality \\
($\hat{p}\in [0, 1],\; \hat{r}\in [0, R_{max}]$).\\
\textbf{PAC bound:} With probability $1-\delta$, $R_{max}$ will reach an $\epsilon$-optimal policy in a number of steps that is polynomial in $|X|, |A|, T, 1/\epsilon$ and $log(1/\delta)$. Memory $O(|X|^2|A|)$. 

\subsection{Model-free RL}
Learn $\pi^*$ only via $V^*$ or $Q^{V^*}$

\subsubsection{TD-learning (On-Policy)}
Given a policy $\pi$ we want to learn $V^\pi$\\
$V^\pi(x)=\mathbb{E}_{R\sim r(x,\pi(x)), X'\sim p(\cdot\vert x, \pi(x))}\left[R + \gamma V^\pi(X')\right]$\\
After seeing $(x_{t+1}, r_t \vert x_t, \pi(x_t))$ we update: \\
$V_{t+1}(x_t)\gets (1-\alpha_t)V_{t}(x_t) + \alpha_t(r_t + \gamma V_t^\pi(x_{t+1}))$\\
Where $\alpha_t$ is a regulizer term (only 1 samlple)
\textbf{Th:} If $\alpha_t\xrightarrow{RM}0$ then $V\xrightarrow{a.s.}V^\pi$

\subsubsection{Q-learning (Off Policy)}
Given experience we want to learn $Q^* = Q^{V^*}$\\
$Q^*(x,a) = \mathbb{E}_{\substack{R\sim r(x,\pi(x)) \\ X'\sim p(\cdot\vert x, \pi(x))}}\left[R+\gamma \max_{a'} Q^*(X',a')\right]$\\
After seeing $(x_{t+1}, r_t \vert x_t, a_t)$ we update: \\
$Q(x_t,a_t) ${\scriptsize $\gets (1-\alpha_t)Q(x_t,a_t) + \alpha_t(r_t+\gamma \max_{a'}Q(x_{t+1}, a'))$}\\
% $V^*(x)=\underset{a}{max}Q*(x,a)$\\
\textbf{Th:} If $\alpha_t\xrightarrow{RM}0$ then $Q\xrightarrow{a.s.}Q^{*}$\\
\textbf{Optimistic Q learning:}\\
Initialize: $Q(x,a)=\frac{R_{max}}{1-\gamma}\prod_{t=1}^{T_{init}}(1-\alpha_t)^{-1}$\\
Same convergence time as with $R_{max}$. Memory $O(|X||A|)$. Comp: $O(|A|)$.

\section{Parametric RL}
\subsection{Parametric TD-learning}
\subsubsection{TD-learinging as SGD}
TD-learing = 1 sample ($x', r \vert x, \pi(x)$) SGD on:\\
$\bar{l}_2(V;x,r) = \frac{1}{2}\left(V-r-\gamma\mathbb{E}_{x'\sim p(\cdot\vert x, \pi(x))}\left[\hat{V}^\pi(x')\right]\right)^2$\\
1 sample estimate of $\nabla_V \bar{l}_2 = \delta = V-r-\gamma \hat{V}^\pi(x')$\\
$\Rightarrow V\gets V - \alpha_t \delta$ where $V = \hat{V}^\pi(x)$

\subsubsection{TD-parametric}
If $\hat{V}^\pi(x) = V(x,\theta)$ then: \\
$\delta = \left[ V(x;\theta)-r-\gamma V(x';\theta_{old})\right] \nabla_\theta V(x,\theta)$

\subsection{Parametric Q-learining}
$\delta (\theta, \theta_{old}) = \left(Q(x,a;\theta)-r-\gamma\max_{a'} Q(x',a';\theta_{old})\right)$\\
We don't differiantiate with regard to $\theta_{old}$\\
The SGD step is:
$\theta \gets \theta - \alpha_t \delta(\theta, \theta) \nabla_\theta Q(x,a;\theta)$

\textbf{Deep Q Networks (DQN):} Version of Q-learning where we update $Q$ only each batch:\\
{\scriptsize $L(\theta) = \sum_{(x,a,r,x')\in\mathcal{D}}\left(r+\gamma\max_{a'}Q(x',a';\theta_{old})-Q(x,a;\theta)\right)^2$}
\textbf{Double DQN (better):} \\
{\scriptsize $L(\theta) = \sum_{(x,a,r,x')\in\mathcal{D}}\left(r+\gamma Q(x',a^*(\theta);\theta_{old})-Q(x,a;\theta)\right)^2$}\\
where: $a^*(\theta) \doteq \argmax_{a'}Q(x',a'; \theta)$

\subsection{Policy-Search method}
$\pi(x) = \pi(x;\theta); \; r(\tau) = \sum_{t=0}^T\gamma^t r(x_t,a_t)$\\
$J(\theta)\doteq J(\pi(\cdot;\theta))= \mathbb{E}_{\tau \sim \pi_\theta}\left[r(\tau)\right] \simeq \frac{1}{m}\sum_{i=1}^m r(\tau^{(i)})$ \\
with $\pi_\theta (\tau) := p(x_0)\prod_{t=1}^T\pi(a_t\vert x_t; \theta)p(x_{t+1}\vert x_t,a_t)$\\
$\hat{\theta} = \argmax_\theta J(\theta) \Rightarrow$ SGD 

\subsubsection{Theory results:}
{\scriptsize $\nabla_\theta J(\theta) = \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}\left[r(\tau)\right]=\mathbb{E}_{\tau \sim \pi_\theta}\left[r(\tau)\nabla_\theta \log{\pi_\theta(\tau)}\right] =: \clubsuit $}\\
{ $ \clubsuit = \mathbb{E}_{\tau\sim\pi_\theta}\left[r(\tau)\sum_{t=0}^T \nabla_\theta \log\pi(a_t\vert x_t;\theta )\right]$}\\
{$ \clubsuit = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum\limits_{t=0}^T(r(\tau)-b(\tau_{0:t-1})) \nabla_\theta \log\pi(a_t\vert x_t;\theta )\right] $}

\subsubsection{REINFORCE}
$b(\tau_{0:t-1}) = \sum_{t'=0}^{t-1}\gamma^{t'}r_{t'};\; r(\tau) - b(\tau_{0:t-1}) = \gamma^t G_t$ \\
Let $G_t = \sum_{t'=t}^{T}\gamma^{t'-t}r_{t'} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots$ \\
$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T\gamma^t G_t\nabla_\theta \log\pi(a_t\vert x_t;\theta )\right]  $\\
SGD:  $\theta \gets \theta + \eta_t \gamma^tG_t \nabla_\theta \pi(a_t\vert x_t; \theta)\;\; \forall t = 1:T$ 
\textbf{Oss:} $G_t \gets G_t - \frac{1}{T}\sum_{t'=0}^{T-1} G_t' \Rightarrow$ Less Variance


\subsection{Actor Critic}
After seeing $(x,a,r,x')$: \\
$\theta_\pi \gets \theta_\pi + \eta_t Q(x,a,\theta_Q)\nabla_{\theta_\pi}\log \pi(a\vert x; \theta_\pi)$\\
$\theta_Q \gets \theta_Q - \eta_t\delta \nabla_{\theta_Q}Q(x,a;\theta_Q)$\\
$\qquad \delta = Q(x,a;\theta_Q)-r-\gamma Q(x',\pi(x',\theta_\pi),\theta_Q)$

\subsubsection{Theory justification:}
$\mathbb{E}_{\tau_{t+1:\infty}\sim \pi_\theta}\left[G_t \vert a_t,x_t\right] = Q^{\pi_\theta}(x_t, a_t)$\\
{\scriptsize $\nabla_{\theta_\pi} J(\theta_\pi)= \mathbb{E}_{\tau \sim \pi_{\theta_\pi}}\left[\sum_{t=0}^T\gamma^t \nabla_{\theta_\pi} \log\pi(a_t\vert x_t;\theta_\pi ) Q(x_t,a_t)\right] $}\\
{\scriptsize $\nabla_{\theta_\pi} J(\theta_\pi)= \mathbb{E}_{\tau \sim \pi_{\theta_\pi},x\sim \rho(\cdot)}\left[\nabla_{\theta_\pi} \log\pi(a_t\vert x_t;\theta_\pi ) Q(x_t,a_t)\right] $}\\
where $\rho(x) = \sum_{t=0}^\infty \gamma^t p(x_t=x)$.

\subsubsection{Advantage Function Baseline}
We can us $V(x;\theta_V)$ as baseline for $Q(x,a;\theta_Q)$\\
$A^\pi(x,a) \doteq Q^\pi(x,a)-V^\pi(x)$\\
\textbf{Oss:} $\forall\pi \forall x \max_a A^\pi(x,a)\geq 0$\\
\textbf{Oss:} $\pi \text{ optimal } \forall x  \;A^\pi(x,\pi(x))\leq 0$.

\subsubsection{Off policy variation}
{\scriptsize $\hat{\theta}_\pi = \argmax J(\theta); \; J(\theta)=\mathbb{E}_{x\sim\mu}\left[Q(x,\pi(x;\theta);\theta_Q)\right]$}\\
where $\mu$ visits all states. SGD can now be done in Batches and Off-policy also for $\theta_\pi$:\\
$\theta_\pi \gets \theta_\pi + \eta \nabla_{\theta_\pi} \frac{1}{|B|}\sum_{(x,a,r,x')\in B}Q(x,\pi(x,\theta_\pi);\theta_Q)$\\
If $\pi$ is randomized we can usa the reparametrization trick.\\
$J_\lambda (\theta) = J(\theta) + \lambda H(\pi_\theta)$ to encourage exploration







