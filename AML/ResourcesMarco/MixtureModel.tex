% -*- root: Main.tex -*-
% \subsection*{EM for GMM}
% Compute cluster membership weight for each point $x_i$ in cluster k, given $\theta_k=(\mu_k,\Sigma_k)$. $\mathbb{E}[z_k|x_i]= P(z_k=1|x_i; \theta)$ \textbf{E}:  $\gamma_k(\mathbf{x}_i) = \frac{P(z_k=1;\theta_k) P(x_i|z_k=1;\theta_k)}{P(x_i;\theta_k)} = 
% 	\frac{\boldsymbol{\pi}_k \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^K \boldsymbol{\pi}_j \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$
% % 	\item[M:] $\sum_{i=1}^{n} \log P(x_i,z_i;\theta)=\\
% % 	\sum_{i=1}^{n} \log[\sum_{k=1}^{K} \pi_k P(x_i|z_i;\theta_k)] =
% % 	\sum_{i=1}^{n} \log[\sum_{k=1}^{K} \gamma_k(x_i)\frac{\pi_k P(x_i|z_i;\theta_k)}{\gamma_k(x_i)}] \geq
% % 	\sum\limits_{i=1}^{n} \sum\limits_{k=1}^{K}\gamma_k(x_i)[\log P(x_i|z_i;\theta_k) + \log \pi_k - \log \gamma_k(x_i)]$\\
% % 	$\frac{\partial}{\partial \pi_k} \sum\limits_{i=1}^{n} \sum\limits_{k=1}^{K}\gamma_k(x_i)[\log P(x_i|z_i;\theta_k) + \log \pi_k - \log \gamma_k(x_i)] + \lambda (\sum\limits_{j=1}^{K} \pi_j -1) \stackrel{\text{!}}{=} 0 \Leftrightarrow \pi_k = \sum\limits_{i=1}^{N} \frac{ \gamma_k(x_i)}{-\lambda}$;$ \sum\limits_{k=1}^{K} \pi_k = 1 =\sum_{k,n=1}^{K,N} \gamma_k(\mathbf{x}_i)\frac{1}{-\lambda} \Leftrightarrow \lambda = N$
% % 	$\boldsymbol{\mu}_k := \frac{\sum_{n=1}^N \gamma_k(x_i) \mathbf{x}_n}{\sum_{n=1}^N \gamma_k(x_i)}$, and $\Sigma_k = \frac{\sum_{n=1}^N \gamma_k(x_i) (\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_k - \boldsymbol{\mu}_k)^T}{\sum_{n=1}^N \gamma_k(x_i)}$
% \textbf{M}: $(\mu^*,\Sigma^*) = \argmax_\theta \mathbb{E}_{\gamma}(\log[p(x|\theta)]) = \argmax_\theta \sum_{i=1}^{n}{\gamma_i(\log[p(x_i|\theta)])}$. $\frac{\partial}{\partial\mu},\frac{\partial}{\partial\Sigma}=0\rightarrow \boldsymbol{\mu}_k:=\frac{\sum_{n=1}^N \gamma_k(x_i) \mathbf{x}_n}{\sum_{n=1}^N \gamma_k(x_i)}$, $\Sigma_k = \frac{\sum_{n=1}^N \gamma_k(x_i) (\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_k - \boldsymbol{\mu}_k)^T}{\sum_{n=1}^N \gamma_k(x_i)}$


% \begin{compactdesc}
% 	\item[Assignment variable:] $\mathbf{z}_k \in \{0, 1\}$, $\sum_{k=1}^K \mathbf{z}_k = 1$, $\operatorname{Pr}(\mathbf{z}_k = 1) = \boldsymbol{\pi}_k \Leftrightarrow p(\mathbf{z}) = \prod_{k=1}^K \boldsymbol{\pi}_k^{\mathbf{z}_k}$, $\pi_k=$mixing prop. of cluster k
% 	\item[Complete data distribution:] $p(\mathbf{x}, \mathbf{z}) = \prod_{k=1}^K \left( \boldsymbol{\pi}_k \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)^{\mathbf{z}_k}$
% 	\item[Likelihood of observed data (iid) \\ $\mathbf{X=[x_1,..,x_N]}$:] $p(\mathbf{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \prod_{n=1}^N p(\mathbf{x}_n) = \prod_{n=1}^N \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$
% 	\item[Log-likelihood:] $\ln p(\mathbf{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) =\break \sum_{i=1}^N \ln \left( \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)$
% \end{compactdesc}

\section{Mixtures Models (Unsupervised Learning)}
\subsection*{K-means}
We find $\mu_1, \dots, \mu_k$ such that our predictions are $c(x):\mathbbm{R}^d\rightarrow \left\{1,\dots,k\right\}$. \\ 
Find $c(\cdot)$ and $\mu_i\forall i$ that minimize: \\
$\mathcal{R}^{km}(c,\mu_i \forall i) = \sum_x \norm{x-\mu_{c(x)}}^2$\\
\begin{algorithm}[H]
     Initialize $\mu_i \forall i$\;
     \While{$\mu_i$ are changing}{
        $c(x) \gets \argmin_c \norm{x-\mu_c}^2 \;\forall x$\;
        $\mu_\alpha = \frac{1}{n_\alpha} \sum_{x:c(x) = \alpha}x \;\forall \alpha$\;
     }
\end{algorithm}
\subsection*{Gaussian Mixtures}
	1) Draw $z\sim\pi$ Categorical.\\ 
	2) Draw $x\sim N(\mu_z, \Sigma_z)$
	
\subsection*{Expectation Maximization}

\begin{algorithm}[H]
	Initialize $\theta^0 = \pi^0, \mu^0, \sigma^{2\, 0} $\;
	\While{$\norm{\theta^{j+1}-\theta^j}>\epsilon$ }{
	   E-step: \\
	   $\gamma_{xc}\doteq\mathbbm{E}\left[M_{xc}\vert X, \theta^j\right] = 
	   \frac{p(X\vert c, \theta^j), p(c\vert\theta^j)}{p(x\vert \theta^j)} = 
	   \frac{N(\mu^j_c,\sigma^{2\,j}_c)\pi_c^j}{\sum_\nu \pi_\nu N(\mu_\nu, \sigma^{2\,j}_{\nu})}$ \\
	   $Q(\theta, \theta_j)=\mathbbm{E}\left[L(X,X_L\vert \theta)\vert \theta_j \right] = 
	   \sum_{x \in X} \sum_{c} \left(\gamma_{xc} \log(\pi_c P(x\vert \theta_c))\right)$\\
	   M-step: 
	   $\theta_{j+1} = \argmax_\theta Q(\theta, \theta_j)$\\
	   $\pi^{j+1}_c = \frac{1}{\vert X \vert} \sum_{x\in X}\gamma_{xc}$\\
	   $\mu^{j+1}_c = \frac{\sum_{x\in X}\gamma_{xc}x}{\sum_{x\in X}\gamma_{xc}}$\\
	   $\sigma^{2\,j+1}_{c} = \frac{\sum_{x\in X}\gamma_{xc}(x-\mu_c)^2}{\sum_{x\in X}\gamma_{xc}}$\\
	}
\end{algorithm}
Where $M_{xc} = \mathbbm{I}_{\left\{x \text{ generated by }c\right\}}(x))$