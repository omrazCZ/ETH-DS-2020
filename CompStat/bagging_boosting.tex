\section{Bagging-Boosting}
\subsection{Bootstrap Aggregating (Bagging)}
Bagging is a variance reduction technique.
We need a simple estimator $\hat g$ (for instance a tree). \\
1) We take $B$ bootstrap samples \\
2) For each bootstrap sample we train an estimator $\hat g_1,\dots,\hat g_B$. \\
3) Aggregation: $\hat g_{bag}(\cdot) = \frac{1}{B}\sum_{i=1}^B\hat g_i(\cdot)$ \\
\textbf{classification:} in this case we have majority voting (or we can average base estimator probs of being in each class)\\
\textbf{Subsample aggregating (Subagging):} Like Bagging but we just take subset of the dataset (without repetition) instead of bootstrapping. $m < n$ size of the subsets\\
\textbf{Out-of-Bag Error:}
Some bags have not trained on a particular sample. Can predict this only by the bags that have not been trained on it (should be $\sim 1/3$) for all samples and average to get a valid estimate for the test error. \\
\textbf{Random Forests:} 
Essentially bagged trees. Have $B$ bootstrap samples $\to$ create trees. They reduce dependence between tree estimates by only allowing a random subset of predictors at each split. Default: regression $p/3$, classification $\sqrt{p}$. (in R option mtry).
\begin{tabular}{|l|l|l|l|}
\hline
                 & Tree & Bagging & Random Forrest \\ \hline
Performance      & -    & +       & ++             \\ \hline
Computation      & +    & -       & +/-            \\ \hline
Interpretation   & +    & -       & -              \\ \hline
Out-of-bag error & -    & +       & +              \\ \hline
\end{tabular}
\begin{codebox}{r}{Random forest}
library(randomForest)
rf <- randomForest(y ~ . , data = mydata, mtry=p-1, importance = TRUE, ntree = 100)
# mtry: no. of features from which we chose a split. 
# if mtry = n_predictors then we have bagging trees
oob <- mean((rf$predicted-y.train)^2) #OOB Reg.
oob <- rf$err.rate #Classification
\end{codebox}
\subsection{Boosting}
Boosting is a bias reduction technique. We have $\hat g$ to be very simple estimator (stamp or small tree). We iteratively train a $\hat g$ predictor on the current model and then update the model: $f \gets f + \nu g$. ($\nu$ usually is $.1$) The other parameter is $M$.
